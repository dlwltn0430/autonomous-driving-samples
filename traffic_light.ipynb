{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZUxj/Rx4ipUy1Ah4Vr7+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlwltn0430/autonomous-driving-samples/blob/main/traffic_light.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# 데이터 경로 설정\n",
        "train_dir = 'dataset/train'\n",
        "validation_dir = 'dataset/validation'\n",
        "\n",
        "# 이미지 데이터 제너레이터 설정\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 데이터 제너레이터 생성\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 모델 생성\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# 모델 저장\n",
        "model.save('traffic_light_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FzKoFxb6bFO",
        "outputId": "7858e5b4-3b8b-4430-8c28-09fc452eb8f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80 images belonging to 3 classes.\n",
            "Found 43 images belonging to 3 classes.\n",
            "Epoch 1/25\n",
            "2/2 [==============================] - 9s 5s/step - loss: 0.8596 - accuracy: 0.5208 - val_loss: 0.8504 - val_accuracy: 0.4375\n",
            "Epoch 2/25\n",
            "2/2 [==============================] - 7s 5s/step - loss: 0.7415 - accuracy: 0.5417 - val_loss: 0.7240 - val_accuracy: 0.4688\n",
            "Epoch 3/25\n",
            "2/2 [==============================] - 6s 4s/step - loss: 0.6906 - accuracy: 0.5417 - val_loss: 0.6355 - val_accuracy: 1.0000\n",
            "Epoch 4/25\n",
            "2/2 [==============================] - 7s 4s/step - loss: 0.6767 - accuracy: 0.6719 - val_loss: 0.6136 - val_accuracy: 0.5312\n",
            "Epoch 5/25\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6883 - accuracy: 0.6250 - val_loss: 0.5272 - val_accuracy: 1.0000\n",
            "Epoch 6/25\n",
            "2/2 [==============================] - 8s 6s/step - loss: 0.6118 - accuracy: 0.7083 - val_loss: 0.4039 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.4633 - accuracy: 0.7500 - val_loss: 0.2719 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "2/2 [==============================] - 7s 4s/step - loss: 0.4458 - accuracy: 0.8281 - val_loss: 0.2373 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.4151 - accuracy: 0.8281 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "2/2 [==============================] - 8s 3s/step - loss: 0.3848 - accuracy: 0.8750 - val_loss: 0.1134 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "2/2 [==============================] - 8s 5s/step - loss: 0.2425 - accuracy: 0.8750 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.2396 - accuracy: 0.8750 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "2/2 [==============================] - 8s 4s/step - loss: 0.4339 - accuracy: 0.8333 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "2/2 [==============================] - 5s 4s/step - loss: 0.3447 - accuracy: 0.8333 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "2/2 [==============================] - 7s 5s/step - loss: 0.2954 - accuracy: 0.8333 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.1146 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "2/2 [==============================] - 7s 4s/step - loss: 0.2138 - accuracy: 0.9583 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "2/2 [==============================] - 11s 7s/step - loss: 0.2265 - accuracy: 0.9062 - val_loss: 0.0558 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "2/2 [==============================] - 9s 2s/step - loss: 0.2283 - accuracy: 0.8542 - val_loss: 9.1798e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "2/2 [==============================] - 5s 4s/step - loss: 0.1349 - accuracy: 0.9375 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "2/2 [==============================] - 7s 3s/step - loss: 0.1066 - accuracy: 0.9792 - val_loss: 0.1989 - val_accuracy: 0.9375\n",
            "Epoch 22/25\n",
            "2/2 [==============================] - 6s 3s/step - loss: 0.1982 - accuracy: 0.8906 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "2/2 [==============================] - 8s 3s/step - loss: 0.1959 - accuracy: 0.9375 - val_loss: 8.3631e-04 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "2/2 [==============================] - 7s 4s/step - loss: 0.1046 - accuracy: 0.9531 - val_loss: 0.1061 - val_accuracy: 0.9688\n",
            "Epoch 25/25\n",
            "2/2 [==============================] - 6s 2s/step - loss: 0.1150 - accuracy: 0.9375 - val_loss: 0.0832 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model('traffic_light_model.h5')\n",
        "\n",
        "# 클래스 레이블\n",
        "class_labels = ['Red', 'Yellow', 'Green']\n",
        "\n",
        "def classify_traffic_light(image_path):\n",
        "    # 이미지 로드\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (150, 150))\n",
        "    image = img_to_array(image) / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # 예측\n",
        "    prediction = model.predict(image)\n",
        "    class_idx = np.argmax(prediction)\n",
        "    class_label = class_labels[class_idx]\n",
        "\n",
        "    return class_label\n",
        "\n",
        "# 예시 이미지 분류\n",
        "image_path = 'dataset/test/red_light.jpg'\n",
        "result = classify_traffic_light(image_path)\n",
        "print(f\"The traffic light is: {result}\")\n",
        "\n",
        "image_path2 = 'dataset/test/green_light.jpg'\n",
        "result2 = classify_traffic_light(image_path2)\n",
        "print(f\"The traffic light is: {result2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f312plaA6cr8",
        "outputId": "20a0dee1-35ea-46a8-ad8a-79cbc4722103"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 124ms/step\n",
            "The traffic light is: Yellow\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "The traffic light is: Green\n"
          ]
        }
      ]
    }
  ]
}